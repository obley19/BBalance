from DrissionPage import ChromiumPage, ChromiumOptions
from multiprocessing import Process, Lock, Event
import json
import time
import random
import os
import urllib.parse
import math
import shutil

# ================== C·∫§U H√åNH ==================
NUM_PROCESSES = 3 
PAGES_PER_KW = 100

KEYWORDS = [
    "ƒë√®n bi c·∫ßu led mini", "c·∫£m bi·∫øn √°p su·∫•t l·ªëp van trong", "t·∫•m c√°ch √¢m ch·ªëng ·ªìn √¥ t√¥", 
    "b∆°m l·ªëp √¥ t√¥ ƒëi·ªán t·ª≠", "b√∫t x√≥a v·∫øt x∆∞·ªõc s∆°n √¥ t√¥", "dung d·ªãch t·∫©y ·ªë k√≠nh xe", 
    "ch·ªëng ƒë·ªï xe m√°y", "g√°c ch√¢n sau nh√¥m ƒë√∫c", "l·ªçc gi√≥ tr·ª• K&N", "bugi iridium", 
    "c√πm tƒÉng t·ªëc domino", "bao tay RCB ch√≠nh h√£ng", "nh√¥ng s√™n dƒ©a DID",
    "m√°y l·ªçc n∆∞·ªõc cho m√®o inox", "m√°y s·∫•y l√¥ng th√∫ c∆∞ng", "th·ª©c ƒÉn h·∫°t s·∫•y kh√¥ l·∫°nh", 
    "gel dinh d∆∞·ª°ng cho m√®o", "thu·ªëc tr·ªã n·∫•m cho ch√≥ m√®o", "b·ªâm cho ch√≥ ƒë·ª±c", 
    "c·ªè m√®o t∆∞∆°i", "ƒë·ªì ch∆°i th√¥ng minh cho ch√≥", "v√≤ng c·ªï GPS th√∫ c∆∞ng", 
    "thu·ªëc x·ªãt h∆∞·ªõng d·∫´n v·ªá sinh ƒë√∫ng ch·ªó", "pate t∆∞∆°i handmade cho th√∫ c∆∞ng",
    "s√°ch artbook game anime", "s√°ch tri·∫øt h·ªçc nh·∫≠p m√¥n", "v·ªü vi·∫øt calligraphy", 
    "b√∫t th∆∞ ph√°p Nh·∫≠t B·∫£n", "m·ª±c vi·∫øt m√°y cao c·∫•p Iroshizuku", "con d·∫•u s√°p seal wax", 
    "gi·∫•y v·∫Ω watercolor 300gsm", "k·ªá s√°ch g·ªó th√¥ng mini", "ƒë√®n ƒë·ªçc s√°ch ch·ªëng c·∫≠n", 
    "t√∫i tote v·∫£i canvas ƒë·ª±ng s√°ch", "b√†n ph√≠m c∆° custom", "keycap resin th·ªß c√¥ng", 
    "chu·ªôt gaming kh√¥ng d√¢y si√™u nh·∫π", "l√≥t chu·ªôt kh·ªï l·ªõn 90x40", "ƒë√®n led treo m√†n h√¨nh", 
    "m√¥ h√¨nh nh√¢n v·∫≠t t·ª∑ l·ªá 1/12", "b·ªô kit th√™u ch·ªØ th·∫≠p", "len s·ª£i milk cotton", 
    "m√°y in ·∫£nh l·∫•y li·ªÅn", "b·ªô m√†u marker 80 m√†u", "con quay gi·∫£m stress metal",
    "c√¢y th·ªßy t√πng ƒë·ªÉ b√†n", "sen ƒë√° kim c∆∞∆°ng", "c√¢y n·∫Øp ·∫•m b·∫Øt m·ªìi", 
    "h·ªá th·ªëng t∆∞·ªõi nh·ªè gi·ªçt t·ª± ƒë·ªông", "ƒë√®n led quang ph·ªï cho c√¢y", "ƒë·∫•t nung Akadama", 
    "ph√¢n b√≥n tan ch·∫≠m Nh·∫≠t B·∫£n", "k√©o t·ªâa c√†nh bonsai", "m√°y ƒëo ƒë·ªô ·∫©m ƒë·∫•t",
    "b√†n x·∫øp nh√¥m d√£ ngo·∫°i", "gh·∫ø x·∫øp th∆∞ gi√£n Naturehike", "ly gi·ªØ nhi·ªát qu√¢n ƒë·ªôi", 
    "t√∫i ng·ªß ch·ªãu nhi·ªát √¢m ƒë·ªô", "b·∫øp c·ªßi camping g·∫•p g·ªçn", "th√πng gi·ªØ nhi·ªát c√≥ b√°nh xe", 
    "m√°y l·ªçc n∆∞·ªõc c·∫ßm tay du l·ªãch", "d√¢y thun r√†ng ƒë·ªì si√™u b·ªÅn", "m√≥c kh√≥a ƒëa nƒÉng 10 trong 1",
    "module camera ESP32", "m√†n h√¨nh OLED 0.96 inch", "c·∫£m bi·∫øn v√¢n tay Arduino", 
    "b·ªô kit xe t·ª± h√†nh robot", "pin LiPo 3.7V", "tr·∫°m h√†n thi·∫øc ƒëi·ªÅu ch·ªânh nhi·ªát", 
    "ngu·ªìn t·ªï ong 12v 20a", "d√¢y c√°p jumper ƒë·ª±c c√°i", "v·ªè case Raspberry Pi 4",
    "vi√™n u·ªëng b·ªï m·∫Øt ch·ªëng √°nh s√°ng xanh", "k·∫πo d·∫ªo gi√∫p ng·ªß ngon melatonin", 
    "b·ªôt protein th·ª±c v·∫≠t", "m√°y massage c·ªï vai g√°y c·∫ßm tay", "m√°y tƒÉm n∆∞·ªõc c·∫ßm tay", 
    "mi·∫øng d√°n m·ª•n hydrocolloid", "x·ªãt kho√°ng l√†m d·ªãu da", "thanh lƒÉn massage m·∫∑t ƒë√° c·∫©m th·∫°ch"
]

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data_shopee")
os.makedirs(DATA_DIR, exist_ok=True)
OUTPUT_FILE = os.path.join(DATA_DIR, "shopee_tong_hop.jsonl") 

file_lock = Lock()

def load_existing_links():
    seen = set()
    if not os.path.exists(OUTPUT_FILE): return seen
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    link = data.get("link", "")
                    if link: seen.add(link.split('?')[0])
                except: continue
    except: pass
    return seen

def run_browser_worker(worker_id, keywords_chunk, start_event):
    print(f"ü§ñ Worker {worker_id}: ƒêang kh·ªüi t·∫°o m√¥i tr∆∞·ªùng...")
    local_seen = load_existing_links()
    
    co = ChromiumOptions()
    port = 9330 + worker_id 
    co.set_local_port(port)
    
    # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY ---
    try:
        co.headless(False) # C√∫ ph√°p ƒë√∫ng cho b·∫£n m·ªõi
    except AttributeError:
        co.set_argument('--headless=new') if False else None # D·ª± ph√≤ng cho b·∫£n r·∫•t c≈©
        
    co.set_argument('--start-maximized')
    co.set_argument('--no-sandbox')
    co.set_argument('--mute-audio')
    
    user_data_path = os.path.abspath(os.path.join(BASE_DIR, f"UserData_W{worker_id}"))
    co.set_user_data_path(user_data_path)

    try:
        page = ChromiumPage(addr_or_opts=co)
        print(f"üåê Worker {worker_id}: ƒêang m·ªü Shopee t·∫°i Port {port}...")
        page.get("https://shopee.vn")
        
        start_event.wait() 
        
        for kw in keywords_chunk:
            print(f"üîç Worker {worker_id}: T√¨m ki·∫øm '{kw}'")
            low_quality_streak = 0
            
            for page_num in range(PAGES_PER_KW):
                encoded_kw = urllib.parse.quote(kw)
                url = f"https://shopee.vn/search?keyword={encoded_kw}&page={page_num}"
                
                try:
                    page.get(url)
                    time.sleep(random.uniform(4, 6))
                    
                    if "shopee.vn/buyer/login" in page.url or page.ele('text:ƒêƒÉng nh·∫≠p'):
                        print(f"‚ö†Ô∏è Worker {worker_id}: C·∫ßn x·ª≠ l√Ω Captcha/Login. ƒêang t·∫°m d·ª´ng...")
                        time.sleep(15)
                        page.refresh()

                    for _ in range(4):
                        page.scroll.down(1200)
                        time.sleep(0.8)
                    page.scroll.to_bottom()
                    time.sleep(1)

                    links = page.eles('css:a[href*="-i."]')
                    if len(links) < 5:
                        low_quality_streak += 1
                        if low_quality_streak >= 3: break
                        continue
                    else: low_quality_streak = 0

                    buffer = []
                    count = 0
                    for item in links:
                        try:
                            href = item.attr('href')
                            if not href: continue
                            if "http" not in href: href = "https://shopee.vn" + href
                            clean_link = href.split('?')[0]
                            if clean_link in local_seen: continue
                            
                            lines = [l.strip() for l in item.text.split('\n') if l.strip()]
                            if len(lines) < 2: continue
                            
                            title = lines[1] if any(x in lines[0] for x in ["Y√™u th√≠ch", "Mall", "Ad"]) else lines[0]
                            price = 0
                            sold = "0"
                            for l in lines:
                                if '‚Ç´' in l:
                                    p = l.replace('‚Ç´','').replace('.','').split('-')[0].strip()
                                    if p.isdigit(): price = int(p)
                                if 'ƒê√£ b√°n' in l:
                                    sold = l.replace('ƒê√£ b√°n','').strip()

                            if price > 0:
                                row = {"title": title, "price": price, "sold": sold, "link": clean_link, "keyword": kw, "platform": "Shopee"}
                                buffer.append(json.dumps(row, ensure_ascii=False))
                                local_seen.add(clean_link)
                                count += 1
                        except: continue
                    
                    if buffer:
                        with file_lock:
                            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                                f.write("\n".join(buffer) + "\n")
                    
                    print(f"‚úÖ W{worker_id} | {kw} (P{page_num+1}): L·∫•y ƒë∆∞·ª£c {count} m√≥n.")
                except Exception as e:
                    print(f"‚ùå W{worker_id} l·ªói: {e}")
                    time.sleep(5)
            
    except Exception as e:
        print(f"‚ùå Worker {worker_id} ch·∫øt: {e}")
    finally:
        try: page.quit()
        except: pass

if __name__ == "__main__":
    chunk_size = math.ceil(len(KEYWORDS) / NUM_PROCESSES)
    keyword_chunks = [KEYWORDS[i:i + chunk_size] for i in range(0, len(KEYWORDS), chunk_size)]
    start_event = Event()
    
    print("="*60)
    print(f"üî• KH·ªûI CH·∫†Y H·ªÜ TH·ªêNG C√ÄO SHOPEE - 3 TR√åNH DUY·ªÜT")
    print("="*60)
    
    processes = []
    for i in range(NUM_PROCESSES):
        if i < len(keyword_chunks):
            p = Process(target=run_browser_worker, args=(i+1, keyword_chunks[i], start_event))
            processes.append(p)
            p.start()
            time.sleep(3) 
            
    print("\nüëâ ƒêANG M·ªû TR√åNH DUY·ªÜT... VUI L√íNG ƒê·ª¢I.")
    input("üëâ SAU KHI C√ÅC C·ª¨A S·ªî ƒê√É HI·ªÜN L√äN, NH·∫§N [ENTER] ƒê·ªÇ CH·∫†Y...")
    
    start_event.set()
    for p in processes:
        p.join()
    print("\nüèÜ HO√ÄN TH√ÄNH.")