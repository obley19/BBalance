"""
Parser module for HTML processing and Vietnamese word segmentation.
Merged from processor/cleaner.py - Data cleaning and text normalization.
"""

import re
import html
from pyvi import ViTokenizer  # Th∆∞ vi·ªán t√°ch t·ª´ ti·∫øng Vi·ªát


class DataCleaner:
    """Class for cleaning and normalizing Vietnamese text data."""
    
    def __init__(self):
        # Regex ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c th·∫ª HTML (VD: <div>, <br>, </a>)
        self.html_tag_re = re.compile(r'<[^>]+>')
        
        # Regex ƒë·ªÉ t√¨m c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng ph·∫£i ch·ªØ s·ªë ho·∫∑c ch·ªØ c√°i ti·∫øng Vi·ªát/Anh
        # Gi·ªØ l·∫°i kho·∫£ng tr·∫Øng ƒë·ªÉ kh√¥ng b·ªã d√≠nh ch·ªØ
        self.special_char_re = re.compile(r'[^\w\s\dƒëƒêa-zA-Z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπ]')

    def clean_html(self, raw_html: str) -> str:
        """
        Lo·∫°i b·ªè to√†n b·ªô th·∫ª HTML v√† decode c√°c k√Ω t·ª± entity (VD: &amp; -> &)
        
        Args:
            raw_html: Raw HTML string
            
        Returns:
            Cleaned text without HTML tags
        """
        if not raw_html:
            return ""
        
        # 1. Unescape HTML entities (v√≠ d·ª•: &nbsp; -> space)
        text = html.unescape(raw_html)
        
        # 2. X√≥a th·∫ª HTML b·∫±ng Regex
        text = self.html_tag_re.sub(' ', text)
        
        # 3. X√≥a kho·∫£ng tr·∫Øng th·ª´a (VD: "  iPhone   " -> "iPhone")
        text = ' '.join(text.split())
        
        return text

    def normalize_text(self, text: str) -> str:
        """
        Chu·∫©n h√≥a vƒÉn b·∫£n ƒë·ªÉ Indexing (Search Engine c·∫ßn c√°i n√†y)
        Input: "ƒêi·ªán tho·∫°i iPhone 14 Pro Max!"
        Output: "ƒëi·ªán_tho·∫°i iphone 14 pro max"
        
        Args:
            text: Raw Vietnamese text
            
        Returns:
            Normalized and segmented text
        """
        if not text:
            return ""
        
        # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
        text = text.lower()
        
        # 2. L√†m s·∫°ch r√°c HTML tr∆∞·ªõc (ƒë·ªÅ ph√≤ng)
        text = self.clean_html(text)
        
        # 3. T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng PyVi (Quan tr·ªçng cho Semantic Search)
        # VD: "t√≠nh nƒÉng" -> "t√≠nh_nƒÉng"
        text = ViTokenizer.tokenize(text)
        
        return text
    
    def remove_boilerplate(self, text: str) -> str:
        """Remove common boilerplate text from product descriptions."""
        boilerplate = "Gi√° s·∫£n ph·∫©m tr√™n Tiki ƒë√£ bao g·ªìm thu·∫ø"
        if boilerplate in text:
            return text.split(boilerplate)[0].strip()
        return text


def parse_html(html_content: str) -> dict:
    """
    Parse HTML content and extract structured data.
    
    Args:
        html_content: Raw HTML string
        
    Returns:
        Dictionary containing extracted data
    """
    cleaner = DataCleaner()
    return {"text": cleaner.clean_html(html_content)}


def segment_vietnamese(text: str) -> str:
    """
    Perform Vietnamese word segmentation using PyVi.
    
    Args:
        text: Raw Vietnamese text
        
    Returns:
        Segmented text with words separated by underscores
    """
    return ViTokenizer.tokenize(text)


def clean_text(text: str) -> str:
    """
    Clean text by removing HTML tags, scripts, and extra whitespace.
    
    Args:
        text: Raw text with potential HTML artifacts
        
    Returns:
        Cleaned text
    """
    cleaner = DataCleaner()
    return cleaner.clean_html(text)


# --- Ph·∫ßn Test ch·∫°y th·ª≠ ---
if __name__ == "__main__":
    cleaner = DataCleaner()
    
    # Test v·ªõi ƒëo·∫°n m√¥ t·∫£ b·∫©n
    dirty_desc = """<h5>N·ªôi dung qu·∫£ng c√°o</h5>\n<p>iPhone 14 Pro Max. B·∫Øt tr·ªçn chi ti·∫øt.</p>"""
    
    print("üîª G·ªêC:", dirty_desc)
    print("‚úÖ SAU KHI CLEAN:", cleaner.clean_html(dirty_desc))
    
    dirty_title = "ƒêi·ªán tho·∫°i iPhone 14 Pro Max - H√†ng Ch√≠nh H√£ng"
    print("\nüîª G·ªêC:", dirty_title)
    print("‚úÖ SAU KHI NORMALIZE (Cho Indexer):", cleaner.normalize_text(dirty_title))
