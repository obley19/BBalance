import requests
import json
import time
import os
import random
from datetime import datetime

# Import module Cleaner b·∫°n v·ª´a t·∫°o ·ªü b∆∞·ªõc tr∆∞·ªõc
# L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o file src/processor/__init__.py ƒë√£ t·ªìn t·∫°i
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
from src.processor.cleaner import DataCleaner

class TikiSpider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://tiki.vn/',
        }
        self.cleaner = DataCleaner()
        self.output_file = 'data/processed/tiki_products.jsonl'
        
        # T·∫°o folder data/processed n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)

    def crawl_product_detail(self, product_id):
        """L·∫•y chi ti·∫øt 1 s·∫£n ph·∫©m (nh∆∞ POC ƒë√£ l√†m)"""
        try:
            url = f"https://tiki.vn/api/v2/products/{product_id}"
            resp = requests.get(url, headers=self.headers)
            if resp.status_code == 200:
                return resp.json()
            return None
        except Exception:
            return None

    def parse_and_save(self, raw_data):
        """L√†m s·∫°ch v√† l∆∞u d·ªØ li·ªáu ƒë√∫ng chu·∫©n Schema"""
        if not raw_data:
            return

        # 1. Clean Data
        clean_title = self.cleaner.normalize_text(raw_data.get('name'))
        clean_desc = self.cleaner.clean_html(raw_data.get('description'))
        
        # 2. Map theo Schema (docs/data_schema.md)
        product = {
            "id": f"tiki_{raw_data.get('id')}",
            "original_id": raw_data.get('id'),
            "title": raw_data.get('name', ''),
            "price": raw_data.get('price', -1),
            "url": f"https://tiki.vn/{raw_data.get('url_path', '')}",
            "image_url": raw_data.get('thumbnail_url', ''),
            "category": raw_data.get('categories', {}).get('name', 'Unknown'),
            "source": "tiki",
            "rating": raw_data.get('rating_average', 0),
            "sold_count": raw_data.get('all_time_quantity_sold', 0),
            "description_clean": clean_desc, # D√πng cho search
            "title_clean": clean_title,      # D√πng cho search
            "crawled_at": datetime.now().isoformat()
        }

        # 3. L∆∞u (Append mode)
        with open(self.output_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(product, ensure_ascii=False) + '\n')
        
        print(f"‚úÖ Saved: {product['id']} - {product['title'][:30]}...")

    def crawl_category(self, category_id, max_pages=3):
        """
        Crawl to√†n b·ªô s·∫£n ph·∫©m trong 1 danh m·ª•c
        API Listing: https://tiki.vn/api/personalish/v1/blocks/listings
        """
        base_url = "https://tiki.vn/api/personalish/v1/blocks/listings"
        
        for page in range(1, max_pages + 1):
            print(f"\n--- üìÑ Page {page} (Category {category_id}) ---")
            params = {
                'limit': 40,            # 40 s·∫£n ph·∫©m/trang
                'include': 'advertisement',
                'aggregations': 2,
                'trackity_id': '78d3810d-275d-8523-2868-e04732162985',
                'category': category_id,
                'page': page,
                'urlKey': 'dien-thoai-may-tinh-bang' # Ch·ªâ l√† key gi·∫£ ƒë·ªÉ API ch·∫°y
            }
            
            try:
                resp = requests.get(base_url, headers=self.headers, params=params)
                if resp.status_code == 200:
                    items = resp.json().get('data', [])
                    if not items:
                        print("‚ö†Ô∏è H·∫øt s·∫£n ph·∫©m, d·ª´ng crawl.")
                        break
                    
                    # L·∫∑p qua t·ª´ng s·∫£n ph·∫©m trong trang danh s√°ch
                    for item in items:
                        pid = item.get('id')
                        # G·ªçi API chi ti·∫øt ƒë·ªÉ l·∫•y full description
                        detail = self.crawl_product_detail(pid)
                        self.parse_and_save(detail)
                        
                        # Sleep nh·∫π ƒë·ªÉ tr√°nh b·ªã block IP
                        time.sleep(random.uniform(0.5, 1.5))
                else:
                    print(f"‚ùå L·ªói trang danh m·ª•c: {resp.status_code}")
            except Exception as e:
                print(f"‚ùå L·ªói h·ªá th·ªëng: {e}")

if __name__ == "__main__":
    spider = TikiSpider()
    
    # ID 1789: ƒêi·ªán tho·∫°i - M√°y t√≠nh b·∫£ng
    # ID 1882: ƒêi·ªán gia d·ª•ng
    # ID 8322: Nh√† s√°ch Tiki
    target_categories = [1789, 1882, 8322]
    
    for cat_id in target_categories:
        print(f"\nüöÄ B·∫Øt ƒë·∫ßu crawl danh m·ª•c: {cat_id}")
        spider.crawl_category(cat_id, max_pages=1) # Test th·ª≠ 1 trang m·ªói lo·∫°i