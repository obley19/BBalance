# T√ÄI LI·ªÜU K·ª∏ THU·∫¨T: GI·∫¢I TH√çCH CHI TI·∫æT H·ªÜ TH·ªêNG CRAWLER

## ƒê·ªì √°n: E-Commerce Search Engine (SEG301)

---

## M·ª•c l·ª•c

1. [T·ªïng quan ki·∫øn tr√∫c](#1-t·ªïng-quan-ki·∫øn-tr√∫c)
2. [K·ªπ thu·∫≠t Anti-Bot (Ch·ªëng ch·∫∑n)](#2-k·ªπ-thu·∫≠t-anti-bot-ch·ªëng-ch·∫∑n)
3. [Chu·∫©n h√≥a d·ªØ li·ªáu (Data Schema)](#3-chu·∫©n-h√≥a-d·ªØ-li·ªáu-data-schema)
4. [L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning)](#4-l√†m-s·∫°ch-vƒÉn-b·∫£n-text-cleaning)
5. [X·ª≠ l√Ω gi√° ti·ªÅn (Price Normalization)](#5-x·ª≠-l√Ω-gi√°-ti·ªÅn-price-normalization)
6. [Kh·ª≠ tr√πng l·∫∑p (De-duplication)](#6-kh·ª≠-tr√πng-l·∫∑p-de-duplication)

---

## 1. T·ªïng quan ki·∫øn tr√∫c

H·ªá th·ªëng Crawler ƒë∆∞·ª£c thi·∫øt k·∫ø theo m√¥ h√¨nh **Abstract Factory Pattern**, v·ªõi m·ªôt l·ªõp c∆° s·ªü (`BaseSpider`) ƒë·ªãnh nghƒ©a interface chung cho t·∫•t c·∫£ c√°c spider.

### C·∫•u tr√∫c th∆∞ m·ª•c

```
src/crawler/
‚îú‚îÄ‚îÄ base_spider.py      # L·ªõp c∆° s·ªü cho t·∫•t c·∫£ spider
‚îú‚îÄ‚îÄ async_base_spider.py # L·ªõp c∆° s·ªü cho spider b·∫•t ƒë·ªìng b·ªô
‚îú‚îÄ‚îÄ schema.py           # ƒê·ªãnh nghƒ©a ProductItem schema
‚îú‚îÄ‚îÄ parser.py           # X·ª≠ l√Ω HTML v√† t√°ch t·ª´ ti·∫øng Vi·ªát
‚îú‚îÄ‚îÄ utils.py            # C√°c h√†m ti·ªán √≠ch (User-Agent, config)
‚îî‚îÄ‚îÄ spiders/
    ‚îú‚îÄ‚îÄ shopee_spider.py
    ‚îú‚îÄ‚îÄ tiki_spider.py
    ‚îú‚îÄ‚îÄ chotot_async_spider.py
    ‚îî‚îÄ‚îÄ ebay_async_spider.py
```

---

## 2. K·ªπ thu·∫≠t Anti-Bot (Ch·ªëng ch·∫∑n)

### 2.1. Xoay v√≤ng User-Agent (User-Agent Rotation)

**File:** `src/crawler/utils.py`

```python
# Common User-Agents for rotating
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
]

def get_random_user_agent() -> str:
    """Return a random User-Agent string."""
    return random.choice(USER_AGENTS)
```

**Gi·∫£i th√≠ch:**

- **V·∫•n ƒë·ªÅ:** C√°c s√†n TMƒêT ki·ªÉm tra header `User-Agent` ƒë·ªÉ ph√°t hi·ªán bot. N·∫øu d√πng User-Agent m·∫∑c ƒë·ªãnh c·ªßa th∆∞ vi·ªán (`python-requests/2.x`), s·∫Ω b·ªã block ngay l·∫≠p t·ª©c.
- **Gi·∫£i ph√°p:** T·∫°o danh s√°ch c√°c User-Agent ph·ªï bi·∫øn t·ª´ tr√¨nh duy·ªát th·∫≠t (Chrome, Firefox, Safari), sau ƒë√≥ ch·ªçn ng·∫´u nhi√™n m·ªói l·∫ßn g·ª≠i request.
- **Hi·ªáu qu·∫£:** Gi·∫£ l·∫≠p h√†nh vi c·ªßa ng∆∞·ªùi d√πng th·ª±c, tr√°nh b·ªã ƒë√°nh d·∫•u l√† bot.

---

### 2.2. Rate Limiting (Gi·ªõi h·∫°n t·ªëc ƒë·ªô)

**File:** `src/crawler/base_spider.py`

```python
# Platform specific configurations
PLATFORM_CONFIGS = {
    "shopee": {"rate_limit": 2.0, "base_url": "https://shopee.vn"},
    "tiki":   {"rate_limit": 1.0, "base_url": "https://tiki.vn"},
    "chotot": {"rate_limit": 0.5, "base_url": "https://www.chotot.com"},
    "ebay":   {"rate_limit": 1.5, "base_url": "https://www.ebay.com"}
}

def sleep_random(self) -> None:
    """Sleep for a random duration to avoid rate limiting."""
    sleep_time = random.uniform(self.rate_limit * 0.5, self.rate_limit * 1.5)
    time.sleep(sleep_time)
```

**Gi·∫£i th√≠ch:**

- **V·∫•n ƒë·ªÅ:** G·ª≠i request li√™n t·ª•c v·ªõi t·ªëc ƒë·ªô cao s·∫Ω k√≠ch ho·∫°t h·ªá th·ªëng b·∫£o v·ªá c·ªßa s√†n (v√≠ d·ª•: Shopee block sau ~10 request/gi√¢y).
- **Gi·∫£i ph√°p:**
  - C·∫•u h√¨nh `rate_limit` ri√™ng cho t·ª´ng s√†n (Shopee: 2s, Tiki: 1s, Ch·ª£ T·ªët: 0.5s, eBay: 1.5s)
  - Th√™m ƒë·ªô ng·∫´u nhi√™n (`random.uniform`) ƒë·ªÉ pattern kh√¥ng b·ªã l·∫∑p l·∫°i
- **C√¥ng th·ª©c:** `sleep_time = rate_limit √ó (0.5 ƒë·∫øn 1.5)`
  - V√≠ d·ª•: Shopee c√≥ rate_limit = 2.0 ‚Üí sleep t·ª´ 1-3 gi√¢y gi·ªØa c√°c request

---

### 2.3. Gi·∫£ l·∫≠p tr√¨nh duy·ªát th·∫≠t (Browser Automation)

**File:** `src/crawler/spiders/shopee_spider.py`

```python
from DrissionPage import ChromiumPage, ChromiumOptions

def _init_browser(self):
    co = ChromiumOptions()
    # Random port to avoid conflicts
    port = 9330 + random.randint(0, 100) 
    co.set_local_port(port)
    
    # C·∫•u h√¨nh tr√¨nh duy·ªát
    co.headless(False)              # Ch·∫°y c√≥ giao di·ªán (d·ªÖ debug)
    co.set_argument('--start-maximized')
    co.set_argument('--no-sandbox')
    co.set_argument('--mute-audio')
    
    self.page = ChromiumPage(addr_or_opts=co)
```

**Gi·∫£i th√≠ch:**

- **V·∫•n ƒë·ªÅ:** Shopee s·ª≠ d·ª•ng JavaScript rendering v√† anti-bot r·∫•t m·∫°nh. Th∆∞ vi·ªán `requests` kh√¥ng th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c v√¨ kh√¥ng ch·∫°y JavaScript.
- **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng `DrissionPage` (wrapper c·ªßa Chromium) ƒë·ªÉ:
  - Ch·∫°y JavaScript ƒë·∫ßy ƒë·ªß
  - Cookie v√† session ƒë∆∞·ª£c qu·∫£n l√Ω t·ª± ƒë·ªông
  - Fingerprint gi·ªëng tr√¨nh duy·ªát th·∫≠t
- **Random port:** M·ªói l·∫ßn kh·ªüi ƒë·ªông d√πng port kh√°c nhau ƒë·ªÉ tr√°nh conflict khi ch·∫°y nhi·ªÅu instance.

---

### 2.4. Ph√°t hi·ªán v√† x·ª≠ l√Ω CAPTCHA

**File:** `src/crawler/spiders/shopee_spider.py`

```python
# Check Login/Captcha
if "shopee.vn/buyer/login" in self.page.url or self.page.ele('text:ƒêƒÉng nh·∫≠p'):
    print(f"‚ö†Ô∏è Login/Captcha detected. Pausing 15s...")
    time.sleep(15)  # Cho ng∆∞·ªùi d√πng t·ª± gi·∫£i captcha
    self.page.refresh()
```

**Gi·∫£i th√≠ch:**

- **Nh·∫≠n di·ªán:** Ki·ªÉm tra URL c√≥ ch·ª©a `/buyer/login` ho·∫∑c c√≥ element text "ƒêƒÉng nh·∫≠p"
- **X·ª≠ l√Ω:** Pause 15 gi√¢y ƒë·ªÉ ng∆∞·ªùi d√πng gi·∫£i CAPTCHA th·ªß c√¥ng (trong tr∆∞·ªùng h·ª£p ch·∫°y c√≥ giao di·ªán)
- **Sau ƒë√≥:** Refresh trang v√† ti·∫øp t·ª•c crawl

---

### 2.5. Cu·ªôn trang ƒë·ªÉ load lazy-loading content

**File:** `src/crawler/spiders/shopee_spider.py`

```python
# Scroll to load lazy items
for _ in range(4):
    self.page.scroll.down(1200)  # Cu·ªôn xu·ªëng 1200px
    time.sleep(0.8)              # ƒê·ª£i JavaScript load
self.page.scroll.to_bottom()     # Cu·ªôn t·ªõi cu·ªëi trang
time.sleep(1)
```

**Gi·∫£i th√≠ch:**

- **V·∫•n ƒë·ªÅ:** Shopee d√πng k·ªπ thu·∫≠t Lazy Loading - ch·ªâ load s·∫£n ph·∫©m khi ng∆∞·ªùi d√πng cu·ªôn t·ªõi v√πng hi·ªÉn th·ªã.
- **Gi·∫£i ph√°p:** M√¥ ph·ªèng h√†nh vi cu·ªôn trang:
  1. Cu·ªôn t·ª´ t·ª´ (4 l·∫ßn √ó 1200px = 4800px)
  2. ƒê·ª£i 0.8s m·ªói l·∫ßn ƒë·ªÉ JavaScript k·ªãp load
  3. Cu·ªôn t·ªõi cu·ªëi trang ƒë·ªÉ ƒë·∫£m b·∫£o load h·∫øt s·∫£n ph·∫©m

---

## 3. Chu·∫©n h√≥a d·ªØ li·ªáu (Data Schema)

### 3.1. ProductItem Schema

**File:** `src/crawler/schema.py`

```python
# --- ƒê·ªäNH NGHƒ®A T√äN TR∆Ø·ªúNG (KH·ªöP 100% V·ªöI CODE CLEAN V30) ---
FIELD_ID = "id"
FIELD_PLATFORM = "platform"
FIELD_TITLE = "title"
FIELD_LINK = "link"
FIELD_IMAGE_URL = "image_url"
FIELD_PRICE = "price"
FIELD_ORIGINAL_PRICE = "original_price"
FIELD_SOLD_COUNT = "sold_count"
FIELD_CATEGORY = "category"
FIELD_BRAND = "brand"

class ProductItem:
    def __init__(self, id, platform, title, price, link, ...):
        # X·ª≠ l√Ω logic an to√†n d·ªØ li·ªáu
        if original_price is None or original_price <= 0:
            original_price = price  # Fallback: d√πng gi√° hi·ªán t·∫°i
            
        # X·ª≠ l√Ω Title: X√≥a k√Ω t·ª± xu·ªëng d√≤ng
        clean_title = title.strip().replace('\n', ' ').replace('\r', '')
        
        # ƒê√≥ng g√≥i v√†o dictionary
        self.data = {
            FIELD_ID: str(id),
            FIELD_TITLE: clean_title,
            FIELD_PRICE: int(price),
            FIELD_PLATFORM: str(platform).lower(),  # Lu√¥n vi·∫øt th∆∞·ªùng
            # ... c√°c field kh√°c
        }
```

**Gi·∫£i th√≠ch:**

- **M·ª•c ƒë√≠ch:** ƒê·∫£m b·∫£o t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ 4 s√†n kh√°c nhau ƒë·ªÅu c√≥ c√πng c·∫•u tr√∫c
- **Field Constants:** D√πng constants (`FIELD_ID`, `FIELD_TITLE`...) thay v√¨ hardcode string ƒë·ªÉ tr√°nh l·ªói typo
- **Validation t·ª± ƒë·ªông:**
  - `original_price`: N·∫øu null ho·∫∑c ‚â§ 0 ‚Üí l·∫•y gi√° hi·ªán t·∫°i
  - `platform`: Lu√¥n convert v·ªÅ lowercase (`Shopee` ‚Üí `shopee`)
  - `title`: X√≥a k√Ω t·ª± newline (`\n`, `\r`)

---

### 3.2. T·∫°o ID duy nh·∫•t

**File:** `src/crawler/base_spider.py`

```python
def build_product_id(self, original_id) -> str:
    """Build unique product ID with platform prefix."""
    return f"{self.source}_{original_id}"
```

**Gi·∫£i th√≠ch:**

- **V·∫•n ƒë·ªÅ:** ID s·∫£n ph·∫©m gi·ªØa c√°c s√†n c√≥ th·ªÉ tr√πng nhau (v√≠ d·ª•: Shopee v√† Tiki ƒë·ªÅu c√≥ product ID = `12345`)
- **Gi·∫£i ph√°p:** Th√™m prefix t√™n s√†n v√†o tr∆∞·ªõc ID
  - Input: `12345` (t·ª´ Shopee)
  - Output: `shopee_12345`
- **L·ª£i √≠ch:** ƒê·∫£m b·∫£o ID unique trong to√†n b·ªô h·ªá th·ªëng khi merge data

---

## 4. L√†m s·∫°ch vƒÉn b·∫£n (Text Cleaning)

### 4.1. Lo·∫°i b·ªè HTML Tags

**File:** `src/crawler/parser.py`

```python
import re
import html

class DataCleaner:
    def __init__(self):
        # Regex ƒë·ªÉ t√¨m t·∫•t c·∫£ c√°c th·∫ª HTML (VD: <div>, <br>, </a>)
        self.html_tag_re = re.compile(r'<[^>]+>')
    
    def clean_html(self, raw_html: str) -> str:
        """
        Lo·∫°i b·ªè to√†n b·ªô th·∫ª HTML v√† decode c√°c k√Ω t·ª± entity
        VD: "&amp;" -> "&", "&nbsp;" -> " "
        """
        if not raw_html:
            return ""
        
        # 1. Unescape HTML entities
        text = html.unescape(raw_html)  # "&amp;" -> "&"
        
        # 2. X√≥a th·∫ª HTML b·∫±ng Regex
        text = self.html_tag_re.sub(' ', text)  # "<div>text</div>" -> " text "
        
        # 3. X√≥a kho·∫£ng tr·∫Øng th·ª´a
        text = ' '.join(text.split())  # "  iPhone   " -> "iPhone"
        
        return text
```

**V√≠ d·ª• th·ª±c t·∫ø:**

| Input | Output |
|-------|--------|
| `<h5>N·ªôi dung qu·∫£ng c√°o</h5><p>iPhone 14 Pro Max.</p>` | `N·ªôi dung qu·∫£ng c√°o iPhone 14 Pro Max.` |
| `ƒêi·ªán tho·∫°i &amp; Ph·ª• ki·ªán` | `ƒêi·ªán tho·∫°i & Ph·ª• ki·ªán` |

---

### 4.2. T√°ch t·ª´ ti·∫øng Vi·ªát (Vietnamese Word Segmentation)

**File:** `src/crawler/parser.py`

```python
from pyvi import ViTokenizer

def normalize_text(self, text: str) -> str:
    """
    Chu·∫©n h√≥a vƒÉn b·∫£n ƒë·ªÉ Indexing
    Input: "ƒêi·ªán tho·∫°i iPhone 14 Pro Max!"
    Output: "ƒëi·ªán_tho·∫°i iphone 14 pro max"
    """
    # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    # 2. L√†m s·∫°ch HTML (ƒë·ªÅ ph√≤ng)
    text = self.clean_html(text)
    
    # 3. T√°ch t·ª´ ti·∫øng Vi·ªát b·∫±ng PyVi
    # VD: "t√≠nh nƒÉng" -> "t√≠nh_nƒÉng"
    text = ViTokenizer.tokenize(text)
    
    return text
```

**Gi·∫£i th√≠ch s·ª± quan tr·ªçng c·ªßa t√°ch t·ª´:**

| Input | Kh√¥ng t√°ch t·ª´ | C√≥ t√°ch t·ª´ (PyVi) |
|-------|---------------|-------------------|
| "m√°y t√≠nh x√°ch tay" | `["m√°y", "t√≠nh", "x√°ch", "tay"]` | `["m√°y_t√≠nh", "x√°ch_tay"]` |
| "ƒëi·ªán tho·∫°i th√¥ng minh" | `["ƒëi·ªán", "tho·∫°i", "th√¥ng", "minh"]` | `["ƒëi·ªán_tho·∫°i", "th√¥ng_minh"]` |

**T·∫°i sao c·∫ßn t√°ch t·ª´?**

- Ti·∫øng Vi·ªát l√† ng√¥n ng·ªØ ƒë∆°n l·∫≠p (isolating language)
- M·ªôt t·ª´ c√≥ nghƒ©a th∆∞·ªùng g·ªìm 2+ √¢m ti·∫øt: "m√°y t√≠nh" = 1 t·ª´, kh√¥ng ph·∫£i 2 t·ª´
- N·∫øu kh√¥ng t√°ch t·ª´, search "m√°y t√≠nh" s·∫Ω kh·ªõp c·∫£ "m√°y gi·∫∑t" v√† "t√≠nh to√°n" ‚Üí k·∫øt qu·∫£ sai

---

## 5. X·ª≠ l√Ω gi√° ti·ªÅn (Price Normalization)

**File:** `src/processor/normalize_data.py`

```python
import re

def clean_price(price):
    """
    Chuy·ªÉn ƒë·ªïi gi√° t·ª´ nhi·ªÅu format v·ªÅ s·ªë nguy√™n
    VD: "1.200.000ƒë" -> 1200000
        "$99.99" -> 99
    """
    if isinstance(price, (int, float)):
        return int(price)
    
    if isinstance(price, str):
        # X√≥a d·∫•u ch·∫•m v√† ph·∫©y, ch·ªâ gi·ªØ l·∫°i s·ªë
        nums = re.findall(r'\d+', price.replace('.', '').replace(',', ''))
        if nums:
            return int(nums[0])
    
    return 0  # Default n·∫øu kh√¥ng parse ƒë∆∞·ª£c
```

**V√≠ d·ª• x·ª≠ l√Ω:**

| Input | Output | Gi·∫£i th√≠ch |
|-------|-------:|------------|
| `1200000` (int) | `1200000` | ƒê√£ l√† s·ªë nguy√™n |
| `1200000.0` (float) | `1200000` | Chuy·ªÉn float ‚Üí int |
| `"1.200.000ƒë"` | `1200000` | X√≥a d·∫•u ch·∫•m v√† k√Ω t·ª± ƒë |
| `"1,200,000 VND"` | `1200000` | X√≥a d·∫•u ph·∫©y v√† VND |
| `"$99.99"` | `99` | Ch·ªâ l·∫•y ph·∫ßn s·ªë ƒë·∫ßu ti√™n |
| `null` / `""` | `0` | Default value |

---

## 6. Kh·ª≠ tr√πng l·∫∑p (De-duplication)

**File:** `src/processor/deduplicate.py`

```python
def deduplicate_file(filepath: str) -> tuple[int, int]:
    """
    Kh·ª≠ tr√πng l·∫∑p file JSONL, gi·ªØ l·∫°i b·∫£n ghi m·ªõi nh·∫•t
    """
    products = {}  # id -> product (keep latest)
    original_count = 0
    
    # ƒê·ªçc t·∫•t c·∫£ s·∫£n ph·∫©m
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            product = json.loads(line)
            original_count += 1
            
            pid = product.get('id')
            if not pid:
                continue
            
            # Ki·ªÉm tra xem ƒë√£ t·ªìn t·∫°i ch∆∞a
            if pid in products:
                # So s√°nh timestamp, gi·ªØ b·∫£n m·ªõi h∆°n
                existing_time = products[pid].get('crawled_at', '')
                new_time = product.get('crawled_at', '')
                
                if new_time > existing_time:
                    products[pid] = product  # Thay th·∫ø b·∫±ng b·∫£n m·ªõi
            else:
                products[pid] = product  # Th√™m m·ªõi
    
    return original_count, len(products)
```

**Logic ho·∫°t ƒë·ªông:**

1. ƒê·ªçc t·ª´ng d√≤ng trong file JSONL
2. V·ªõi m·ªói s·∫£n ph·∫©m, ki·ªÉm tra `id` ƒë√£ t·ªìn t·∫°i trong dict ch∆∞a
3. N·∫øu tr√πng ID:
   - So s√°nh `crawled_at` (timestamp)
   - Gi·ªØ l·∫°i b·∫£n ghi c√≥ timestamp m·ªõi h∆°n
4. Ghi ƒë√® file v·ªõi d·ªØ li·ªáu ƒë√£ dedupe

**K·∫øt qu·∫£ th·ª±c t·∫ø:**

```
üìä DEDUPLICATION REPORT
==================================================
üìÑ shopee_products.jsonl
   Before: 850,000 | After: 800,284 | Removed: 49,716 (5.8%)
üìÑ tiki_products.jsonl
   Before: 440,000 | After: 435,203 | Removed: 4,797 (1.1%)
==================================================
üìä TOTAL: 1,290,000 ‚Üí 1,235,487 (54,513 duplicates removed)
```

## 7. H·ªá th·ªëng Checkpoint (Resume Crawling)

### 7.1. T·ªïng quan

**V·∫•n ƒë·ªÅ:** Khi crawl h√†ng tri·ªáu s·∫£n ph·∫©m, qu√° tr√¨nh c√≥ th·ªÉ m·∫•t nhi·ªÅu gi·ªù/ng√†y. N·∫øu b·ªã l·ªói gi·ªØa ch·ª´ng (m·∫•t ƒëi·ªán, crash, b·ªã block), ph·∫£i crawl l·∫°i t·ª´ ƒë·∫ßu ‚Üí **l√£ng ph√≠ th·ªùi gian v√† t√†i nguy√™n**.

**Gi·∫£i ph√°p:** H·ªá th·ªëng Checkpoint l∆∞u tr·∫°ng th√°i crawl ƒë·ªÉ c√≥ th·ªÉ **resume t·ª´ ch·ªó d·ª´ng**.

### 7.2. C·∫•u tr√∫c Checkpoint

**File:** `src/crawler/crawl_state.py`

```python
class CrawlState:
    def __init__(self, state_dir: str = "data/crawl_state"):
        self.state_dir = state_dir
        os.makedirs(state_dir, exist_ok=True)
        
        self.checkpoint_file = os.path.join(state_dir, "checkpoint.json")
        self.crawled_ids_file = os.path.join(state_dir, "crawled_ids.txt")
        
        # Load existing state
        self.checkpoint = self._load_checkpoint()
        self.crawled_ids: Set[str] = self._load_crawled_ids()
```

**Hai file ƒë∆∞·ª£c l∆∞u:**

| File | N·ªôi dung | M·ª•c ƒë√≠ch |
|------|----------|----------|
| `checkpoint.json` | Category ƒëang crawl, page hi·ªán t·∫°i | Resume ƒë√∫ng v·ªã tr√≠ |
| `crawled_ids.txt` | Danh s√°ch ID ƒë√£ crawl | Tr√°nh crawl tr√πng |

### 7.3. L∆∞u tr·∫°ng th√°i (Save Checkpoint)

```python
def save_checkpoint(self, platform: str, category: str, page: int) -> None:
    """Save current crawl progress."""
    self.checkpoint = {
        "platform": platform,
        "current_category": category,
        "last_page": page,
        "completed_categories": self.checkpoint.get("completed_categories", []),
        "timestamp": datetime.now().isoformat(),
    }
    with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(self.checkpoint, f, indent=2, ensure_ascii=False)
```

**V√≠ d·ª• n·ªôi dung `checkpoint.json`:**

```json
{
  "platform": "shopee",
  "current_category": "dien-thoai",
  "last_page": 15,
  "completed_categories": [
    "shopee:laptop",
    "shopee:may-tinh-bang"
  ],
  "timestamp": "2024-01-28T10:30:00"
}
```

### 7.4. Kh√¥i ph·ª•c (Resume)

```python
def get_resume_page(self, platform: str, category: str) -> int:
    """Get the page to resume from for a category."""
    if (self.checkpoint.get("platform") == platform and 
        self.checkpoint.get("current_category") == category):
        return self.checkpoint.get("last_page", 0)
    return 0  # B·∫Øt ƒë·∫ßu t·ª´ ƒë·∫ßu n·∫øu kh√¥ng match
```

**Logic ho·∫°t ƒë·ªông:**

1. Khi kh·ªüi ƒë·ªông crawler, load checkpoint t·ª´ file
2. Ki·ªÉm tra category hi·ªán t·∫°i kh·ªõp v·ªõi checkpoint kh√¥ng
3. N·∫øu kh·ªõp ‚Üí Resume t·ª´ `last_page`
4. N·∫øu kh√¥ng ‚Üí B·∫Øt ƒë·∫ßu t·ª´ page 0

### 7.5. Theo d√µi ID ƒë√£ crawl (Duplicate Prevention)

```python
def is_crawled(self, product_id: str) -> bool:
    """Check if a product ID was already crawled."""
    return product_id in self.crawled_ids

def mark_crawled(self, product_id: str) -> None:
    """Mark a product ID as crawled."""
    if product_id not in self.crawled_ids:
        self.crawled_ids.add(product_id)
        # Append to file (atomic operation)
        with open(self.crawled_ids_file, 'a', encoding='utf-8') as f:
            f.write(product_id + '\n')
```

**T·∫°i sao d√πng Set + File?**

| Approach | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|----------|---------|------------|
| **Set (in-memory)** | Tra c·ª©u O(1) | M·∫•t khi restart |
| **File (disk)** | B·ªÅn v·ªØng | Tra c·ª©u ch·∫≠m |
| **Set + File** | Tra c·ª©u nhanh + B·ªÅn v·ªØng | T·ªën RAM cho set |

‚Üí D√πng c·∫£ hai: Set ƒë·ªÉ tra c·ª©u nhanh, File ƒë·ªÉ backup.

---

## 8. C√¢u h·ªèi th∆∞·ªùng g·∫∑p (FAQ - Potential Interview Questions)

### Q1: T·∫°i sao ch·ªçn JSONL thay v√¨ JSON ho·∫∑c CSV?

**Tr·∫£ l·ªùi:**

| Format | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|--------|---------|------------|
| **JSON** | ƒê·ªçc to√†n b·ªô d·ªÖ d√†ng | Ph·∫£i load h·∫øt v√†o RAM, kh√¥ng append ƒë∆∞·ª£c |
| **CSV** | Nh·∫π, d·ªÖ ƒë·ªçc | Kh√¥ng h·ªó tr·ª£ nested data, encoding issues |
| **JSONL** | Append ƒë∆∞·ª£c, memory efficient, h·ªó tr·ª£ nested | M·ªói d√≤ng ph·∫£i parse ri√™ng |

**JSONL ph√π h·ª£p v√¨:**

- Crawler ch·∫°y li√™n t·ª•c, c·∫ßn **append** d·ªØ li·ªáu
- M·ªói d√≤ng ƒë·ªôc l·∫≠p ‚Üí ƒë·ªçc t·ª´ng d√≤ng m√† kh√¥ng c·∫ßn load h·∫øt file
- H·ªó tr·ª£ **streaming processing** cho file l·ªõn (1M+ d√≤ng)

---

### Q2: T·∫°i sao c·∫ßn t√°ch t·ª´ ti·∫øng Vi·ªát (Word Segmentation)?

**Tr·∫£ l·ªùi:**

Ti·∫øng Vi·ªát l√† **ng√¥n ng·ªØ ƒë∆°n l·∫≠p** (isolating language) - c√°c t·ª´ kh√¥ng c√≥ d·∫•u c√°ch r√µ r√†ng gi·ªØa c√°c th√†nh ph·∫ßn.

**V√≠ d·ª• v·∫•n ƒë·ªÅ:**

```
Query: "m√°y t√≠nh"
VƒÉn b·∫£n: "M√°y gi·∫∑t t√≠nh nƒÉng m·ªõi"
```

N·∫øu kh√¥ng t√°ch t·ª´:

- "m√°y" c√≥ trong vƒÉn b·∫£n ‚úì
- "t√≠nh" c√≥ trong vƒÉn b·∫£n ‚úì
- ‚Üí Kh·ªõp sai!

N·∫øu c√≥ t√°ch t·ª´:

- "m√°y_t√≠nh" kh√¥ng c√≥ trong vƒÉn b·∫£n ‚úó
- ‚Üí Kh√¥ng kh·ªõp (ƒë√∫ng!)

---

### Q3: Sync vs Async Crawler - Khi n√†o d√πng c√°i n√†o?

**Tr·∫£ l·ªùi:**

| Ti√™u ch√≠ | Sync (requests) | Async (aiohttp) |
|----------|-----------------|-----------------|
| **T·ªëc ƒë·ªô** | 1 request/l·∫ßn | H√†ng trƒÉm request ƒë·ªìng th·ªùi |
| **Ph·ª©c t·∫°p** | ƒê∆°n gi·∫£n | C·∫ßn hi·ªÉu async/await |
| **Ph√π h·ª£p** | API ch·∫≠m, c·∫ßn browser | API nhanh, nhi·ªÅu endpoint |

**Trong project:**

- **Shopee:** D√πng Sync + Browser (anti-bot m·∫°nh)
- **Ch·ª£ T·ªët/eBay:** D√πng Async (API d·ªÖ, c·∫ßn t·ªëc ƒë·ªô)

---

### Q4: L√†m sao x·ª≠ l√Ω khi b·ªã block IP?

**Tr·∫£ l·ªùi:**

1. **Rate Limiting:** Gi·∫£m t·ªëc ƒë·ªô request
2. **Rotate User-Agent:** Thay ƒë·ªïi fingerprint
3. **Proxy Rotation:** ƒê·ªïi IP (n·∫øu c√≥)
4. **Browser Automation:** D√πng tr√¨nh duy·ªát th·∫≠t (DrissionPage)
5. **Checkpoint:** L∆∞u progress ƒë·ªÉ resume sau

**Trong project ƒë√£ l√†m:**

- ‚úÖ Rate Limiting (sleep random)
- ‚úÖ User-Agent Rotation
- ‚úÖ Browser Automation (Shopee)
- ‚úÖ Checkpoint System
- ‚¨ú Proxy Rotation (ch∆∞a implement)

---

### Q5: T·∫°i sao d√πng DrissionPage thay v√¨ Selenium?

**Tr·∫£ l·ªùi:**

| Ti√™u ch√≠ | Selenium | DrissionPage |
|----------|----------|--------------|
| **Setup** | C·∫ßn WebDriver ri√™ng | D√πng Chrome c√≥ s·∫µn |
| **Anti-detection** | D·ªÖ b·ªã ph√°t hi·ªán | Fingerprint gi·ªëng ng∆∞·ªùi th·∫≠t |
| **Performance** | Ch·∫≠m h∆°n | Nhanh h∆°n |
| **API** | C≈©, verbose | Modern, Pythonic |

DrissionPage wrapper Chrome DevTools Protocol (CDP) ‚Üí √≠t b·ªã ph√°t hi·ªán l√† bot h∆°n.

---

### Q6: Explain the difference between crawled_ids (Set) vs deduplicate.py

**Tr·∫£ l·ªùi:**

| Component | Th·ªùi ƒëi·ªÉm | M·ª•c ƒë√≠ch |
|-----------|-----------|----------|
| `crawled_ids` (Set) | **Runtime** - Khi ƒëang crawl | Tr√°nh crawl c√πng 1 product 2 l·∫ßn trong session |
| `deduplicate.py` | **Post-processing** - Sau khi crawl | X√≥a duplicate t·ª´ nhi·ªÅu session kh√°c nhau |

**V√≠ d·ª•:**

- Session 1: Crawl product A, B, C ‚Üí `crawled_ids = {A, B, C}`
- Session 2: Crawl product B, D, E ‚Üí `crawled_ids = {B, D, E}` (B tr√πng v·ªõi session 1)
- Ch·∫°y `deduplicate.py` ‚Üí Lo·∫°i b·ªè B tr√πng, gi·ªØ version m·ªõi nh·∫•t

---

### Q7: T·∫°i sao ID s·∫£n ph·∫©m c·∫ßn prefix platform?

**Tr·∫£ l·ªùi:**

**V·∫•n ƒë·ªÅ:** C√°c s√†n d√πng ID n·ªôi b·ªô kh√°c nhau, c√≥ th·ªÉ tr√πng.

- Shopee product ID: `12345`
- Tiki product ID: `12345` (tr√πng!)

**Gi·∫£i ph√°p:** Th√™m prefix platform:

- `shopee_12345`
- `tiki_12345`

‚Üí ID unique trong to√†n b·ªô h·ªá th·ªëng, kh√¥ng b·ªã conflict khi merge data.

---

### Q8: Memory Management - L√†m sao crawl 1M products kh√¥ng h·∫øt RAM?

**Tr·∫£ l·ªùi:**

1. **JSONL Format:** Ghi t·ª´ng d√≤ng, kh√¥ng gi·ªØ trong RAM
2. **Streaming Write:** Append mode, kh√¥ng load to√†n b·ªô file
3. **Set cho crawled_ids:** Ch·ªâ l∆∞u ID (string ng·∫Øn), kh√¥ng l∆∞u to√†n b·ªô product
4. **Garbage Collection:** Python t·ª± ƒë·ªông gi·∫£i ph√≥ng objects kh√¥ng d√πng

**C√¥ng th·ª©c ∆∞·ªõc t√≠nh RAM:**

```
1M IDs √ó ~30 bytes/ID = ~30MB cho Set
(Ch·∫•p nh·∫≠n ƒë∆∞·ª£c)
```

---

### Q9: Error Handling Strategy

**Tr·∫£ l·ªùi:**

```python
try:
    response = session.get(url, timeout=20)
    if response.status_code != 200:
        print(f"‚ö†Ô∏è HTTP {response.status_code}")
        break  # Stop category, move to next
except Exception as e:
    print(f"‚ùå Error: {e}")
    time.sleep(3)  # Cool down
    # Continue to next page
```

**Strategy:**

1. **Timeout:** Gi·ªõi h·∫°n 20s m·ªói request
2. **HTTP Error:** Log v√† skip
3. **Exception:** Log, sleep, retry ho·∫∑c skip
4. **Checkpoint:** L∆∞u progress ƒë·ªÉ resume

---

## K·∫øt lu·∫≠n

T√†i li·ªáu n√†y ƒë√£ gi·∫£i th√≠ch chi ti·∫øt c√°c k·ªπ thu·∫≠t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·ªá th·ªëng Crawler:

1. **Anti-Bot:** User-Agent rotation, Rate limiting, Browser automation, CAPTCHA handling
2. **Data Schema:** Unified ProductItem v·ªõi field constants
3. **Text Cleaning:** HTML removal, Vietnamese word segmentation
4. **Price Normalization:** Regex-based parsing cho nhi·ªÅu format ti·ªÅn t·ªá
5. **De-duplication:** Dict-based v·ªõi timestamp comparison
6. **Checkpoint System:** Resume crawling, duplicate prevention
7. **FAQ:** C√°c c√¢u h·ªèi k·ªπ thu·∫≠t th∆∞·ªùng g·∫∑p

M·ªói k·ªπ thu·∫≠t ƒë·ªÅu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi·∫£i quy·∫øt m·ªôt v·∫•n ƒë·ªÅ c·ª• th·ªÉ trong qu√° tr√¨nh thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ c√°c s√†n TMƒêT.

---

*T√†i li·ªáu k·ªπ thu·∫≠t - SEG301 E-Commerce Search Engine*
